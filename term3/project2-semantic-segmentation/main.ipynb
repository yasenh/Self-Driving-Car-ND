{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "import warnings\n",
    "from distutils.version import LooseVersion\n",
    "import project_tests as tests\n",
    "import matplotlib.pyplot as plt\n",
    "# Allow image embeding in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path):\n",
    "    \"\"\"\n",
    "    Load Pretrained VGG Model into TensorFlow.\n",
    "    :param sess: TensorFlow Session\n",
    "    :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "    :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "    \"\"\"\n",
    "\n",
    "    vgg_tag = 'vgg16'\n",
    "    vgg_input_tensor_name = 'image_input:0'\n",
    "    vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "    vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "    vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "    vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "\n",
    "    # Use tf.saved_model.loader.load to load the model and weights   \n",
    "    tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)\n",
    "    \n",
    "    # get these layers that we gonna use in next step\n",
    "    vgg_input_tensor = tf.get_default_graph().get_tensor_by_name(vgg_input_tensor_name)\n",
    "    vgg_keep_prob_tensor = tf.get_default_graph().get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "    vgg_layer3_out_tensor = tf.get_default_graph().get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "    vgg_layer4_out_tensor = tf.get_default_graph().get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "    vgg_layer7_out_tensor = tf.get_default_graph().get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "    \n",
    "    return vgg_input_tensor, vgg_keep_prob_tensor, vgg_layer3_out_tensor, vgg_layer4_out_tensor, vgg_layer7_out_tensor\n",
    "\n",
    "#tests.test_load_vgg(load_vgg, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes):\n",
    "    \"\"\"\n",
    "    Create the layers for a fully convolutional network.  Build skip-layers using the vgg layers.\n",
    "    :param vgg_layer7_out: TF Tensor for VGG Layer 3 output\n",
    "    :param vgg_layer4_out: TF Tensor for VGG Layer 4 output\n",
    "    :param vgg_layer3_out: TF Tensor for VGG Layer 7 output\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: The Tensor for the last layer of output\n",
    "    \"\"\"\n",
    "    # reference: http://ethereon.github.io/netscope/#/preset/vgg-16\n",
    "    # https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_segmentation.html\n",
    "    \n",
    "    #conv2d/conv2d_transpose(inputs, filters, kernel_size, strides, padding)\n",
    "    # 1x1 conv instead of fully connected layer\n",
    "    vgg_layer7_conv_out = tf.layers.conv2d(vgg_layer7_out, num_classes, 1, 1,'same')\n",
    "    # 2x upsample\n",
    "    vgg_layer7_transpose = tf.layers.conv2d_transpose(vgg_layer7_conv_out, num_classes, 4, 2, 'same')\n",
    "    vgg_layer4_conv_out = tf.layers.conv2d(vgg_layer4_out, num_classes, 1, 1,'same')\n",
    "    # skip connection\n",
    "    output1 = tf.add(vgg_layer7_transpose, vgg_layer4_conv_out)\n",
    "    # same upsamele and conection \n",
    "    vgg_layer4_transpose = tf.layers.conv2d_transpose(output1, num_classes, 4, 2, 'same')\n",
    "    vgg_layer3_conv_out = tf.layers.conv2d(vgg_layer3_out, num_classes, 1, 1,'same')\n",
    "    output2 = tf.add(vgg_layer4_transpose, vgg_layer3_conv_out)\n",
    "    # 8x upsample back to original image\n",
    "    output = tf.layers.conv2d_transpose(output2, num_classes, 16, 8, 'same')\n",
    "        \n",
    "    return output\n",
    "\n",
    "#tests.test_layers(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate, num_classes):\n",
    "    \"\"\"\n",
    "    Build the TensorFLow loss and optimizer operations.\n",
    "    :param nn_last_layer: TF Tensor of the last layer in the neural network\n",
    "    :param correct_label: TF Placeholder for the correct label image\n",
    "    :param learning_rate: TF Placeholder for the learning rate\n",
    "    :param num_classes: Number of classes to classify\n",
    "    :return: Tuple of (logits, train_op, cross_entropy_loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # reshape to column = num_classes = 2\n",
    "    logits = tf.reshape(nn_last_layer, [-1, num_classes])\n",
    "    labels = tf.reshape(correct_label, [-1, num_classes])\n",
    "    # calculate cross entropy\n",
    "    cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits))\n",
    "    # add to summary for tensorboard\n",
    "    tf.summary.scalar('loss', cross_entropy_loss)\n",
    "    \n",
    "    # Optimizer - Adam works better than SGD\n",
    "    #train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "    \n",
    "    return logits, train_op, cross_entropy_loss\n",
    "\n",
    "#tests.test_optimize(optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "             correct_label, keep_prob, learning_rate, merged, train_writer):\n",
    "    \"\"\"\n",
    "    Train neural network and print out the loss during training.\n",
    "    :param sess: TF Session\n",
    "    :param epochs: Number of epochs\n",
    "    :param batch_size: Batch size\n",
    "    :param get_batches_fn: Function to get batches of training data.  Call using get_batches_fn(batch_size)\n",
    "    :param train_op: TF Operation to train the neural network\n",
    "    :param cross_entropy_loss: TF Tensor for the amount of loss\n",
    "    :param input_image: TF Placeholder for input images\n",
    "    :param correct_label: TF Placeholder for label images\n",
    "    :param keep_prob: TF Placeholder for dropout keep probability\n",
    "    :param learning_rate: TF Placeholder for learning rate\n",
    "    \"\"\"\n",
    "    \n",
    "    # log every 2 * 16 = 32 image\n",
    "    log_batch_step = 16 \n",
    "    batches = []\n",
    "    loss_batch = []\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        print('Epochs %d: ' % epoch_i)\n",
    "        print('---------------------')\n",
    "        i = 0\n",
    "        batch_i = 0\n",
    "        for input_img, gt_img in get_batches_fn(batch_size):\n",
    "            feed_dict = {input_image : input_img, correct_label : gt_img, keep_prob : 0.85, learning_rate : 1e-4}\n",
    "            _, l, summary = sess.run([train_op, cross_entropy_loss, merged], feed_dict = feed_dict)\n",
    "    \n",
    "            i += 1\n",
    "            if (i % log_batch_step == 0):\n",
    "                batch_i += 1\n",
    "                print('Minibatch loss at step %d: %f' % (batch_i, l))\n",
    "                # Log batches\n",
    "                previous_batch = batches[-1] if batches else 0\n",
    "                batches.append(log_batch_step + previous_batch)\n",
    "                loss_batch.append(l)\n",
    "                train_writer.add_summary(summary, batch_i)\n",
    "                train_writer.flush()\n",
    "     \n",
    "    # plot batch-loss summary\n",
    "    loss_plot = plt.subplot(111)\n",
    "    loss_plot.set_title('Loss')\n",
    "    loss_plot.plot(batches, loss_batch, 'g')\n",
    "    loss_plot.set_xlim([batches[0], batches[-1]])\n",
    "    \n",
    "    pass\n",
    "\n",
    "#tests.test_train_nn(train_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    num_classes = 2\n",
    "    image_shape = (160, 576)\n",
    "    data_dir = './data'\n",
    "    runs_dir = './runs'\n",
    "    tests.test_for_kitti_dataset(data_dir)\n",
    "\n",
    "    # Download pretrained vgg model\n",
    "    helper.maybe_download_pretrained_vgg(data_dir)\n",
    "\n",
    "    # OPTIONAL: Train and Inference on the cityscapes dataset instead of the Kitti dataset.\n",
    "    # You'll need a GPU with at least 10 teraFLOPS to train on.\n",
    "    #  https://www.cityscapes-dataset.com/\n",
    "\n",
    "    epochs = 10\n",
    "    batch_size = 2\n",
    "    \n",
    "    with tf.Session() as sess:        \n",
    "        # Path to vgg model\n",
    "        vgg_path = os.path.join(data_dir, 'vgg')\n",
    "        # Create function to get batches\n",
    "        get_batches_fn = helper.gen_batch_function(os.path.join(data_dir, 'data_road/training'), image_shape)\n",
    "        \n",
    "        # OPTIONAL: Augment Images for better results\n",
    "        #  https://datascience.stackexchange.com/questions/5224/how-to-prepare-augment-images-for-neural-network\n",
    "\n",
    "        # Build NN using load_vgg, layers, and optimize function\n",
    "        input_image, keep_prob, vgg_layer3_out, vgg_layer4_out, vgg_layer7_out = load_vgg(sess, vgg_path)\n",
    "        nn_last_layer = layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes)\n",
    "        \n",
    "        correct_label = tf.placeholder(tf.float32, (None, None, None, num_classes))\n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        logits, train_op, cross_entropy_loss = optimize(nn_last_layer, correct_label, learning_rate, num_classes)\n",
    "        \n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "        \n",
    "        # initializes global variables in the graph\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train NN using the train_nn function\n",
    "        train_nn(sess, epochs, batch_size, get_batches_fn, train_op, cross_entropy_loss, input_image,\n",
    "                 correct_label, keep_prob, learning_rate, merged, train_writer)\n",
    "        \n",
    "        # TODO: Save inference data using helper.save_inference_samples\n",
    "        helper.save_inference_samples(runs_dir, data_dir, sess, image_shape, logits, keep_prob, input_image)\n",
    "\n",
    "        # OPTIONAL: Apply the trained model to a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n",
      "INFO:tensorflow:Restoring parameters from b'./data/vgg/variables/variables'\n",
      "Epochs 0: \n",
      "---------------------\n",
      "Minibatch loss at step 1: 7.980023\n",
      "Minibatch loss at step 2: 3.605894\n",
      "Minibatch loss at step 3: 2.167993\n",
      "Minibatch loss at step 4: 1.470710\n",
      "Minibatch loss at step 5: 1.394178\n",
      "Minibatch loss at step 6: 1.073156\n",
      "Minibatch loss at step 7: 1.026151\n",
      "Minibatch loss at step 8: 0.894994\n",
      "Minibatch loss at step 9: 0.879914\n",
      "Training Finished. Saving test images to: ./runs/1502896877.2458344\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncIuIhIyCW0QkZBTcIiIh819kudZXOGdj2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4deb402cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
